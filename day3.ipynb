{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question-1: Smart Greeting Function for a GenAI App\n",
        "Write a function named generate_greeting(name) for a GenAI assistant app.\n",
        "\n",
        "Requirements:\n",
        "\n",
        "Accept one positional argument: name (string).\n",
        "If the user gives extra spaces (example: \" chandu \"), your function must clean the name using strip().\n",
        "If the cleaned name is an empty string (example: \" \"), return: \"Hello! Welcome to GenAI.\"\n",
        "Otherwise, return: \"Hello, <Name>! Welcome to GenAI.\" where <Name> must be title-cased using .title().\n",
        "Test cases you must run:\n",
        "\n",
        "generate_greeting(\"Chandu\")\n",
        "generate_greeting(\" akhila \")\n",
        "generate_greeting(\" \")\n"
      ],
      "metadata": {
        "id": "0tMR4xUZEP4P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PknvjLdbEAI2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "20722057-e095-45fc-ec2e-27c4fc2501f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, Chandu! Welcome to GenAI.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#Problem 1\n",
        "def generate_greeting(name):\n",
        "  clean_name=name.strip()\n",
        "  if clean_name==\"\":\n",
        "    return \"Hello! Welcome to GenAI.\"\n",
        "  else:\n",
        "    return f\"Hello, {name.title()}! Welcome to GenAI.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Cases for Problem 1:"
      ],
      "metadata": {
        "id": "7D9bIOYzg5cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_greeting(\"Chandu\")\n",
        "generate_greeting(\" akhila \")\n",
        "generate_greeting(\" \")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "collapsed": true,
        "id": "JlRK4o2Tg5rR",
        "outputId": "b4bb3bdc-5a15-4d68-9efe-0c74acbca186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! Welcome to GenAI.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question-2: Function That Prints a Mini Report (Define + Call)\n",
        "Create a function named daily_genai_report() that prints a clean 4-line report.\n",
        "\n",
        "Requirements:\n",
        "\n",
        "Inside the function, create:\n",
        "\n",
        "model_name = \"gpt\"\n",
        "tokens_used = 120\n",
        "cost_per_token = 0.02\n",
        "Compute total_cost = tokens_used * cost_per_token\n",
        "\n",
        "Print exactly these 4 lines:\n",
        "\n",
        "Model: gpt\n",
        "Tokens Used: 120\n",
        "Cost Per Token: 0.02\n",
        "Total Cost: 2.4\n",
        "Important:\n",
        "\n",
        "Use print() only (no return).\n",
        "Call the function twice."
      ],
      "metadata": {
        "id": "-bxvccathYlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem 2\n",
        "def daily_genai_report():\n",
        "  model_name = \"gpt\"\n",
        "  tokens_used = 120\n",
        "  cost_per_token = 0.02\n",
        "  total_cost = tokens_used * cost_per_token\n",
        "  print(f\"Model:{model_name} \")\n",
        "  print(f\"Tokens Used:{tokens_used}\")\n",
        "  print(f\"Cost Per Token: {cost_per_token}\")\n",
        "  print(f\"Total Cost: { total_cost}\")\n",
        "\n",
        "daily_genai_report()\n",
        "daily_genai_report()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSexTX0XhsTJ",
        "outputId": "142ca602-77ac-4666-b8a1-be264b15560d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model:gpt \n",
            "Tokens Used:120\n",
            "Cost Per Token: 0.02\n",
            "Total Cost: 2.4\n",
            " Model:gpt \n",
            "Tokens Used:120\n",
            "Cost Per Token: 0.02\n",
            "Total Cost: 2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question-3: Positional Arguments and Order Matters (Compute AI Credits)\n",
        "Write a function buy_credits(user_name, credits, price_per_credit).\n",
        "\n",
        "Requirements:\n",
        "\n",
        "Print a receipt in this format:\n",
        "User: <user_name>\n",
        "Credits Purchased: <credits>\n",
        "Price Per Credit: <price_per_credit>\n",
        "Final Amount: <final_amount>\n",
        "final_amount = credits * price_per_credit\n",
        "Use positional arguments only while calling.\n",
        "Test cases:\n",
        "\n",
        "buy_credits(\"Chandu\", 50, 3)\n",
        "buy_credits(\"Akhila\", 120, 2)\n",
        "Call wrongly: buy_credits(\"Chandu\", 3, 50) Add 1–2 comment lines explaining why the output is wrong.\n"
      ],
      "metadata": {
        "id": "8oNGT03QkY15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 3:\n",
        "def buy_credits(user_name, credits, price_per_credit):\n",
        "\n",
        "  final_amount = credits * price_per_credit\n",
        "\n",
        "  print(f\"\"\"\n",
        "  User: {user_name}\n",
        "  Credits Purchased: {credits}\n",
        "  Price Per Credit: {price_per_credit}\n",
        "  Final Amount: {final_amount}\"\"\")"
      ],
      "metadata": {
        "id": "RmoZ_O22kf7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Cases:"
      ],
      "metadata": {
        "id": "QiUi4B_Ylbwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "buy_credits(\"Chandu\", 50, 3)\n",
        "buy_credits(\"Akhila\", 120, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwlQ_UUQlbZk",
        "outputId": "fe34befb-f463-4472-833d-3a026f4190fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  User: Chandu\n",
            "  Credits Purchased: 50\n",
            "  Price Per Credit: 3\n",
            "  Final Amount: 150\n",
            "\n",
            "  User: Akhila\n",
            "  Credits Purchased: 120\n",
            "  Price Per Credit: 2\n",
            "  Final Amount: 240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "buy_credits(\"Chandu\", 3, 50)\n",
        "# This output is wrong because positional arguments depend on order.\n",
        "# Here, 3 is treated as credits and 50 as price per credit,\n",
        "# which is not what the user intended.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF3DSus2l-qC",
        "outputId": "5f91a145-9a1d-4bf7-b560-3d2f2eb76cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  User: Chandu\n",
            "  Credits Purchased: 3\n",
            "  Price Per Credit: 50\n",
            "  Final Amount: 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question-4: return vs print — Build a Reusable Discount System\n",
        "Create two functions for a GenAI subscription discount.\n",
        "\n",
        "Function-1: discount_print(price, discount_percent)\n",
        "\n",
        "Calculates discounted price and prints it\n",
        "Does not return anything\n",
        "Function-2: discount_return(price, discount_percent)\n",
        "\n",
        "Calculates discounted price and returns it\n",
        "Does not print anything\n",
        "Formula: final = price - (price * discount_percent / 100)\n",
        "\n",
        "Proof code (must do):\n",
        "\n",
        "a = discount_print(1000, 10)\n",
        "\n",
        "b = discount_return(1000, 10)\n",
        "\n",
        "Print a and b\n",
        "\n",
        "Add comments explaining:\n",
        "\n",
        "why a is None\n",
        "why b is 900"
      ],
      "metadata": {
        "id": "wQU7VFkqm6jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 4:\n",
        "def discount_print(price, discount_percent):\n",
        "  final = price - (price * discount_percent / 100)\n",
        "  print(f\"Discounted value : {final}\")\n",
        "\n",
        "def discount_return(price, discount_percent):\n",
        "  final = price - (price * discount_percent / 100)\n",
        "  return final\n",
        "\n"
      ],
      "metadata": {
        "id": "h4i9WaJnnFMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Cases:"
      ],
      "metadata": {
        "id": "nud8jaw4nyZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = discount_print(1000, 10)\n",
        "\n",
        "b = discount_return(1000, 10)\n",
        "\n",
        "print(\"a = \",a)\n",
        "print(\"b = \",b)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_Xk1EiWn1OV",
        "outputId": "bba8a9d6-d584-4c47-aab0-9a9b45e6cfc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "900.0\n",
            "a =  None\n",
            "b =  900.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# a is None because discount_print() only prints the value.\n",
        "# It does not return anything, so Python automatically returns None.\n",
        "\n",
        "# b is 900 because discount_return() sends the calculated value\n",
        "# back using the return keyword.\n"
      ],
      "metadata": {
        "id": "AARwIG_ipFv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question-5: Lambda Tasks for Prompt Processing\n",
        "Given:\n",
        "\n",
        "prompts = [\"Write a poem\", \"Summarize\", \"Generate code\", \"Hi\", \"Explain AI\", \"Ok\"]\n",
        "Tasks:\n",
        "\n",
        "Use map() + lambda to create a list of lengths of each prompt.\n",
        "Use filter() + lambda to create a list of prompts with length > 5.\n",
        "Print both lists."
      ],
      "metadata": {
        "id": "L4ZWDwrkpLi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 5:\n",
        "\n",
        "prompts = [\"Write a poem\", \"Summarize\", \"Generate code\", \"Hi\", \"Explain AI\", \"Ok\"]\n",
        "len_list=list(map(lambda x:len(x),prompts))\n",
        "list_prompt = list(filter(lambda x: len(x) > 5, prompts))\n",
        "\n",
        "print(len_list)\n",
        "print(list_prompt)"
      ],
      "metadata": {
        "id": "PKtu05T5pQ2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8937dbb-90dc-409a-fa20-4576de3b7571"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12, 9, 13, 2, 10, 2]\n",
            "['Write a poem', 'Summarize', 'Generate code', 'Explain AI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question-6: Text File Handling — Save and Load Config\n",
        "Write a program that manages a GenAI configuration file named config.txt.\n",
        "\n",
        "Step-1: Write Config\n",
        "\n",
        "Open config.txt in write mode\n",
        "Write exactly:\n",
        "model=gpt\n",
        "temperature=0.7\n",
        "max_tokens=200\n",
        "Close the file\n"
      ],
      "metadata": {
        "id": "3mRKbqpnZSdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6:\n",
        "#Step 1:\n",
        "with open(\"config.txt\", \"w\") as f:\n",
        "  f.write(\"model=gpt\\n\")\n",
        "  f.write(\"temperature=0.7\\n\")\n",
        "  f.write(\"max_tokens=200\\n\")\n",
        "  f.close()\n",
        "#Step 2:\n",
        "with open(\"config.txt\", \"r\") as f:\n",
        "  info=f.read()\n",
        "  print(info)\n",
        "  print(\"Is Closed?\", f.closed)\n",
        "  f.close()\n",
        "  print(\"Is Closed?\", f.closed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgdiChiwZTd1",
        "outputId": "29ea51b9-eae7-4aca-e473-ac7b4f14e244"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model=gpt\n",
            "temperature=0.7\n",
            "max_tokens=200\n",
            "\n",
            "Is Closed? False\n",
            "Is Closed? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question-7: CSV File Handling — Filter Logs by Token Usage\n",
        "You have model_logs.csv with columns: Timestamp,Prompt,Tokens,ResponseTime\n",
        "\n",
        "Task:\n",
        "\n",
        "Use csv.reader\n",
        "Skip header using next(reader)\n",
        "Print only rows where Tokens > 100\n",
        "Count such rows and print:\n",
        "High Token Requests: <count>"
      ],
      "metadata": {
        "id": "Ur2YfAEkceEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7:\n",
        "import csv\n",
        "with open(\"model_logs.csv\", \"r\") as f:\n",
        "   reader=csv.reader(f)\n",
        "   next(reader)\n",
        "   for row in reader:\n",
        "      tok=int(f.Tokens)\n",
        "      if tok>100:\n",
        "        print(row)\n",
        "        count+=1\n",
        "   print(\"High Token Requests:\", count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "sOo7k479cf4l",
        "outputId": "fb44caa1-4ef7-41d5-8aac-280ea2d0f43b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'model_logs.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4138022218.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Question 7:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_logs.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m    \u001b[0mreader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_logs.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question-8: Modules — Build a Prompt Utility Module\n",
        "Create two files.\n",
        "\n",
        "File-1: prompt_utils.py Create:\n",
        "\n",
        "clean_text(text)\n",
        "\n",
        "strip() spaces\n",
        "lowercase\n",
        "return cleaned text\n",
        "is_long_prompt(text)\n",
        "\n",
        "return True if length > 20, else False\n",
        "ile-2: main.py\n",
        "\n",
        "Import using: from prompt_utils import clean_text, is_long_prompt\n",
        "\n",
        "Input: \" Write a detailed blog on Generative AI in simple words \"\n",
        "\n",
        "Print:\n",
        "\n",
        "cleaned text\n",
        "whether it’s long or not"
      ],
      "metadata": {
        "id": "vGsRVRxJgDQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8\n",
        "#creating prompt_utils.py\n",
        "def clean_text(text):\n",
        "  clean_text=text.strip()\n",
        "  return clean_text.lower()\n",
        "\n",
        "def is_long_prompt(text):\n",
        "  if len(text)>20:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "#Creating main.py\n",
        "from prompt_utils import clean_text,is_long_prompt\n",
        "text=\" Write a detailed blog on Generative AI in simple words \"\n",
        "print(\"cleaned text : \",clean_text(text))\n",
        "print(\"whether it’s long :\"is_long_prompt(text))\n"
      ],
      "metadata": {
        "id": "tjtyIn0XgMhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question-9: Exception Handling — Safe Calculator for GenAI Tokens\n",
        "Write a program that takes:\n",
        "\n",
        "total_tokens (input)\n",
        "num_requests (input)\n",
        "Compute: avg_tokens = total_tokens / num_requests\n",
        "\n",
        "Requirements:\n",
        "\n",
        "Handle ValueError for non-numeric input\n",
        "Handle ZeroDivisionError for 0 requests\n",
        "If valid, print:\n",
        "Average Tokens Per Request: <avg_tokens>\n"
      ],
      "metadata": {
        "id": "eU4PNhPOgM4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  total_tokens=int(input(\"Enter total tokens\"))\n",
        "  num_requests=int(input(\"Enter number of requests\"))\n",
        "  avg_tokens = total_tokens / num_requests\n",
        "  print(f\"Average Tokens Per Request: {avg_tokens}\")\n",
        "except ValueError:\n",
        "  print(\"Invalid input\")\n",
        "\n",
        "except ZeroDivisionError:\n",
        "  print(\"Invalid input\")\n",
        "\n",
        "except Exception as e :\n",
        "  print(f\"An Unexpected error:: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgNtOtxSgRiv",
        "outputId": "2c1ce3ca-940c-47fe-8b21-cead6d041342"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter total tokens4\n",
            "Enter number of requests6\n",
            "Average Tokens Per Request: 0.6666666666666666\n"
          ]
        }
      ]
    }
  ]
}